{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "import shutil\n",
    "import argparse\n",
    "import time\n",
    "import logging\n",
    "import math\n",
    "\n",
    "import models.resnet as models\n",
    "from dataset.data import *\n",
    "\n",
    "import torchvision.models.utils as utils\n",
    "# from tensorboardX import SummaryWriter \n",
    "import numpy as np\n",
    "import easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict({\n",
    "        'cmd': 'train',\n",
    "        'data_dir': 'data/',\n",
    "        'arch': 'multi_resnet18_kd',\n",
    "        'dataset': 'cifar100',\n",
    "        'workers': 8,\n",
    "        'epoch': 10,\n",
    "        'start_epoch': 0,\n",
    "        'batch_size': 128,\n",
    "        'lr': 0.1,\n",
    "        'momentum': 0.9,\n",
    "        'weight_decay': 5e-4,\n",
    "        'print_freq': 100,\n",
    "        'resume': '',\n",
    "        'step_ratio': 0.1,\n",
    "        'warm_up': None,\n",
    "        'save_folder': 'save_checkpoints/',\n",
    "        'summary_folder': 'rums_alpha01/',\n",
    "        'eval_every': 1000,\n",
    "        \n",
    "        # kd parameters\n",
    "        'temperature': 3,\n",
    "        'alpha': 0.1,\n",
    "        'beta': 1e-6\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "#     args = parse_args()\n",
    "    save_path = args.save_path = os.path.join(args.save_folder, args.arch)\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    args.logger_file = os.path.join(save_path, 'log_{}.txt'.format(args.cmd))\n",
    "    handlers = [logging.FileHandler(args.logger_file, mode='w'),\n",
    "                logging.StreamHandler()]\n",
    "    logging.basicConfig(level=logging.INFO,\n",
    "                        datefmt='%m-%d-%y %H:%M',\n",
    "                        format='%(asctime)s:%(message)s',\n",
    "                        handlers=handlers)\n",
    "    \n",
    "    if args.cmd == 'train':\n",
    "        logging.info('start training {}'.format(args.arch))\n",
    "        run_training(args)\n",
    "    \n",
    "    elif args.cmd == 'test':\n",
    "        logging.info('start evaluating {} with checkpoints from {}'.format(\n",
    "            args.arch, args.resume))\n",
    "        run_test(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(args):\n",
    "#     writer = SummaryWriter(args.summary_folder)\n",
    "    if args.dataset == 'cifar100':\n",
    "        model = models.__dict__[args.arch](num_classes=100)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    model = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "    # load checkpoint\n",
    "    if args.resume:\n",
    "        if os.path.isfile(args.resume):\n",
    "            logging.info(\"=> loading checkpoint `{}`\".format(args.resume))\n",
    "            checkpoint = torch.load(args.resume)\n",
    "            args.start_epoch = checkpoint['epoch'] + 1\n",
    "            best_prec1 = checkpoint['best_prec1']\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            logging.info('=> loaded checkpoint `{}` (epoch: {})'.format(\n",
    "                args.resume, checkpoint['epoch']))\n",
    "        else:\n",
    "            logging.info('=> no checkpoint found at `{}`'.format(args.resume))\n",
    "            exit()\n",
    "    \n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    #load dataset\n",
    "    if args.dataset == 'cifar100':\n",
    "        test_loader = prepare_cifar100_test_dataset(data_dir=args.data_dir, batch_size=args.batch_size, \n",
    "                                                        num_workers=args.workers)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "    validate(args, test_loader, model, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(args):\n",
    "#     writer = SummaryWriter(args.summary_folder)\n",
    "    if args.dataset == 'cifar100':\n",
    "        model = models.__dict__[args.arch](num_classes=100)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    model = torch.nn.DataParallel(model).cuda()\n",
    "    best_prec1 = 0\n",
    "\n",
    "    if args.resume:\n",
    "        if os.path.isfile(args.resume):\n",
    "            logging.info(\"=> loading checkpoint `{}`\".format(args.resume))\n",
    "            checkpoint = torch.load(args.resume)\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "            best_prec1 = checkpoint['best_prec1']\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            logging.info('=> loaded checkpoint `{}` (epoch: {})'.format(\n",
    "                args.resume, checkpoint['epoch']))\n",
    "        else:\n",
    "            logging.info('=> no checkpoint found at `{}`'.format(args.resume))\n",
    "    \n",
    "    cudnn.benchmark = True\n",
    "    if args.dataset == 'cifar100':\n",
    "        train_loader = prepare_cifar100_train_dataset(data_dir=args.data_dir, batch_size=args.batch_size, \n",
    "                                                        num_workers=args.workers)\n",
    "        test_loader = prepare_cifar100_test_dataset(data_dir=args.data_dir, batch_size=args.batch_size, \n",
    "                                                        num_workers=args.workers)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "   \n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay = args.weight_decay)\n",
    "\n",
    "\n",
    "    end = time.time()\n",
    "    model.train()\n",
    "    step = 0\n",
    "    for current_epoch in range(args.start_epoch, args.epoch):\n",
    "        batch_time = AverageMeter()\n",
    "        data_time = AverageMeter()\n",
    "        losses = AverageMeter()\n",
    "        top1 = AverageMeter()\n",
    "        middle1_losses = AverageMeter()\n",
    "        middle2_losses = AverageMeter()\n",
    "        middle3_losses = AverageMeter()\n",
    "        losses1_kd = AverageMeter()\n",
    "        losses2_kd = AverageMeter()\n",
    "        losses3_kd = AverageMeter()\n",
    "        feature_losses_1 = AverageMeter()\n",
    "        feature_losses_2 = AverageMeter()\n",
    "        feature_losses_3 = AverageMeter()\n",
    "        total_losses = AverageMeter()\n",
    "        middle1_top1 = AverageMeter()\n",
    "        middle2_top1 = AverageMeter()\n",
    "        middle3_top1 = AverageMeter()\n",
    "\n",
    "        adjust_learning_rate(args, optimizer, current_epoch)\n",
    "        for i, (input, target) in enumerate(train_loader):\n",
    "            data_time.update(time.time() - end)\n",
    "            \n",
    "            target = target.squeeze().long().cuda(non_blocking=True)\n",
    "            input = Variable(input).cuda()\n",
    "            \n",
    "            # all outputs from a model\n",
    "            output, middle_output1, middle_output2, middle_output3, \\\n",
    "            final_fea, middle1_fea, middle2_fea, middle3_fea = model(input)\n",
    "            \n",
    "            # cross entropy loss on a final classifier\n",
    "            loss = criterion(output, target)\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            \n",
    "            # cross entropy losses on shallow classifiers\n",
    "            middle1_loss = criterion(middle_output1, target)\n",
    "            middle1_losses.update(middle1_loss.item(), input.size(0))\n",
    "            middle2_loss = criterion(middle_output2, target)\n",
    "            middle2_losses.update(middle2_loss.item(), input.size(0))\n",
    "            middle3_loss = criterion(middle_output3, target)\n",
    "            middle3_losses.update(middle3_loss.item(), input.size(0))\n",
    "\n",
    "            # output - logit\n",
    "            temp4 = output / args.temperature\n",
    "            temp4 = torch.softmax(temp4, dim=1)\n",
    "            \n",
    "            # KL loss between students and a teacher\n",
    "            loss1by4 = kd_loss_function(middle_output1, temp4.detach(), args) * (args.temperature**2)\n",
    "            losses1_kd.update(loss1by4, input.size(0))\n",
    "            \n",
    "            loss2by4 = kd_loss_function(middle_output2, temp4.detach(), args) * (args.temperature**2)\n",
    "            losses2_kd.update(loss2by4, input.size(0))\n",
    "            \n",
    "            loss3by4 = kd_loss_function(middle_output3, temp4.detach(), args) * (args.temperature**2)\n",
    "            losses3_kd.update(loss3by4, input.size(0))\n",
    "            \n",
    "            # Feature L2 loss\n",
    "            feature_loss_1 = feature_loss_function(middle1_fea, final_fea.detach()) \n",
    "            feature_losses_1.update(feature_loss_1, input.size(0))\n",
    "            feature_loss_2 = feature_loss_function(middle2_fea, final_fea.detach()) \n",
    "            feature_losses_2.update(feature_loss_2, input.size(0))\n",
    "            feature_loss_3 = feature_loss_function(middle3_fea, final_fea.detach()) \n",
    "            feature_losses_3.update(feature_loss_3, input.size(0))\n",
    "\n",
    "            # Total loss\n",
    "            total_loss = (1 - args.alpha) * (loss + middle1_loss + middle2_loss + middle3_loss) + \\\n",
    "                        args.alpha * (loss1by4 + loss2by4 + loss3by4) + \\\n",
    "                        args.beta * (feature_loss_1 + feature_loss_2 + feature_loss_3)\n",
    "            total_losses.update(total_loss.item(), input.size(0))\n",
    "            \n",
    "            prec1 = accuracy(output.data, target, topk=(1,))\n",
    "            top1.update(prec1[0], input.size(0))\n",
    "\n",
    "            middle1_prec1 = accuracy(middle_output1.data, target, topk=(1,))\n",
    "            middle1_top1.update(middle1_prec1[0], input.size(0))\n",
    "            middle2_prec1 = accuracy(middle_output2.data, target, topk=(1,))\n",
    "            middle2_top1.update(middle2_prec1[0], input.size(0))\n",
    "            middle3_prec1 = accuracy(middle_output3.data, target, topk=(1,))\n",
    "            middle3_top1.update(middle3_prec1[0], input.size(0))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            if i % args.print_freq == 0:\n",
    "                step += 1\n",
    "                logging.info(\"Epoch: [{0}]\\t\"\n",
    "                            \"Iter: [{1}]\\t\"\n",
    "                            \"Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\"\n",
    "                            \"Data {data_time.val:.3f} ({data_time.avg:.3f})\\t\"\n",
    "                            \"Loss {loss.val:.3f} ({loss.avg:.3f})\\t\"\n",
    "                            \"Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t\".format(\n",
    "                                current_epoch,\n",
    "                                i,\n",
    "                                batch_time=batch_time,\n",
    "                                data_time=data_time,\n",
    "                                loss=total_losses,\n",
    "                                top1=top1)\n",
    "                ) \n",
    "        prec1 = validate(args, test_loader, model, criterion, None, current_epoch)\n",
    "        is_best = prec1 > best_prec1\n",
    "        best_prec1 = max(prec1, best_prec1)\n",
    "        print(\"best: \", best_prec1)\n",
    "        checkpoint_path = os.path.join(args.save_path, 'checkpoint_{:05d}.pth.tar'.format(current_epoch))\n",
    "        save_checkpoint({\n",
    "            'epoch': current_epoch,\n",
    "            'arch': args.arch,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_prec1': best_prec1,\n",
    "            }, is_best, filename=checkpoint_path)\n",
    "        shutil.copyfile(checkpoint_path, os.path.join(args.save_path, 'checkpoint_latest.pth.tar'))\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(args, test_loader, model, criterion, writer=None, current_epoch=0):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    middle1_losses = AverageMeter()\n",
    "    middle2_losses = AverageMeter()\n",
    "    middle3_losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    middle1_top1 = AverageMeter()\n",
    "    middle2_top1 = AverageMeter()\n",
    "    middle3_top1 = AverageMeter()\n",
    "    model.eval()\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(test_loader):\n",
    "\n",
    "        target = target.squeeze().long().cuda(non_blocking=True)\n",
    "        input = Variable(input).cuda()\n",
    "\n",
    "        output, middle_output1, middle_output2, middle_output3, \\\n",
    "        final_fea, middle1_fea, middle2_fea, middle3_fea = model(input)\n",
    "            \n",
    "        loss = criterion(output, target)\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        middle1_loss = criterion(middle_output1, target)\n",
    "        middle1_losses.update(middle1_loss.item(), input.size(0))\n",
    "        middle2_loss = criterion(middle_output2, target)\n",
    "        middle2_losses.update(middle2_loss.item(), input.size(0))\n",
    "        middle3_loss = criterion(middle_output3, target)\n",
    "        middle3_losses.update(middle3_loss.item(), input.size(0))\n",
    "            \n",
    "        prec1 = accuracy(output.data, target, topk=(1,))\n",
    "        top1.update(prec1[0], input.size(0))\n",
    "        middle1_prec1 = accuracy(middle_output1.data, target, topk=(1,))\n",
    "        middle1_top1.update(middle1_prec1[0], input.size(0))\n",
    "        middle2_prec1 = accuracy(middle_output2.data, target, topk=(1,))\n",
    "        middle2_top1.update(middle2_prec1[0], input.size(0))\n",
    "        middle3_prec1 = accuracy(middle_output3.data, target, topk=(1,))\n",
    "        middle3_top1.update(middle3_prec1[0], input.size(0))\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "    logging.info(\"Loss {loss.avg:.3f}\\t\"\n",
    "                 \"Prec@1 {top1.avg:.3f}\\t\"\n",
    "                 \"Middle1@1 {middle1_top1.avg:.3f}\\t\"\n",
    "                 \"Middle2@1 {middle2_top1.avg:.3f}\\t\"\n",
    "                 \"Middle3@1 {middle3_top1.avg:.3f}\\t\".format(\n",
    "                    loss=losses,\n",
    "                    top1=top1,\n",
    "                    middle1_top1=middle1_top1,\n",
    "                    middle2_top1=middle2_top1,\n",
    "                    middle3_top1=middle3_top1))\n",
    "    \n",
    "    model.train()\n",
    "    return top1.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kd_loss_function(output, target_output,args):\n",
    "    \"\"\"Compute kd loss\"\"\"\n",
    "    \"\"\"\n",
    "    para: output: middle ouptput logits.\n",
    "    para: target_output: final output has divided by temperature and softmax.\n",
    "    \"\"\"\n",
    "\n",
    "    output = output / args.temperature\n",
    "    output_log_softmax = torch.log_softmax(output, dim=1)\n",
    "    loss_kd = -torch.mean(torch.sum(output_log_softmax * target_output, dim=1))\n",
    "    return loss_kd\n",
    "\n",
    "def feature_loss_function(fea, target_fea):\n",
    "    loss = (fea - target_fea)**2 * ((fea > 0) | (target_fea > 0)).float()\n",
    "    return torch.abs(loss).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def adjust_learning_rate(args, optimizer, epoch):\n",
    "    if args.warm_up and (epoch < 1):\n",
    "        lr = 0.01\n",
    "    elif 75 <= epoch < 130:\n",
    "        lr = args.lr * (args.step_ratio ** 1)\n",
    "    elif 130 <= epoch < 180:\n",
    "        lr = args.lr * (args.step_ratio ** 2)\n",
    "    elif epoch >=180:\n",
    "        lr = args.lr * (args.step_ratio ** 3)\n",
    "    else:\n",
    "        lr = args.lr\n",
    "\n",
    "    \n",
    "    logging.info('Epoch [{}] learning rate = {}'.format(epoch, lr))\n",
    "    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.topk(maxk, 1, True, True)  \n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))  \n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul(100.0 / batch_size))\n",
    "    \n",
    "    return res\n",
    "\n",
    "def save_checkpoint(state, is_best, filename):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        save_path = os.path.dirname(filename)\n",
    "        shutil.copyfile(filename, os.path.join(save_path, 'model_best.path.tar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11-26-20 17:55:start training multi_resnet18_kd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11-26-20 17:55:Epoch [0] learning rate = 0.1\n",
      "11-26-20 17:55:Epoch: [0]\tIter: [0]\tTime 0.794 (0.794)\tData 0.197 (0.197)\tLoss 29.264 (29.264)\tPrec@1 0.000 (0.000)\t\n",
      "11-26-20 17:55:Epoch: [0]\tIter: [100]\tTime 0.050 (0.057)\tData 0.000 (0.002)\tLoss 26.898 (28.019)\tPrec@1 4.688 (4.525)\t\n",
      "11-26-20 17:55:Epoch: [0]\tIter: [200]\tTime 0.050 (0.054)\tData 0.000 (0.001)\tLoss 25.939 (27.246)\tPrec@1 9.375 (6.394)\t\n",
      "11-26-20 17:55:Epoch: [0]\tIter: [300]\tTime 0.051 (0.053)\tData 0.000 (0.001)\tLoss 26.137 (26.781)\tPrec@1 7.812 (7.722)\t\n",
      "11-26-20 17:55:Loss 3.550\tPrec@1 14.500\tMiddle1@1 14.340\tMiddle2@1 14.240\tMiddle3@1 13.220\t\n",
      "11-26-20 17:55:Epoch [1] learning rate = 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best:  tensor(14.5000, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11-26-20 17:55:Epoch: [1]\tIter: [0]\tTime 1.969 (1.969)\tData 1.938 (1.938)\tLoss 25.760 (25.760)\tPrec@1 8.594 (8.594)\t\n",
      "11-26-20 17:56:Epoch: [1]\tIter: [100]\tTime 0.051 (0.070)\tData 0.000 (0.020)\tLoss 23.804 (24.744)\tPrec@1 20.312 (15.377)\t\n",
      "11-26-20 17:56:Epoch: [1]\tIter: [200]\tTime 0.051 (0.060)\tData 0.000 (0.010)\tLoss 24.077 (24.425)\tPrec@1 17.188 (16.787)\t\n",
      "11-26-20 17:56:Epoch: [1]\tIter: [300]\tTime 0.051 (0.057)\tData 0.000 (0.007)\tLoss 22.777 (24.115)\tPrec@1 27.344 (18.376)\t\n",
      "11-26-20 17:56:Loss 3.136\tPrec@1 23.330\tMiddle1@1 21.090\tMiddle2@1 22.550\tMiddle3@1 22.420\t\n",
      "11-26-20 17:56:Epoch [2] learning rate = 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best:  tensor(23.3300, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11-26-20 17:56:Epoch: [2]\tIter: [0]\tTime 1.896 (1.896)\tData 1.860 (1.860)\tLoss 22.876 (22.876)\tPrec@1 28.906 (28.906)\t\n",
      "11-26-20 17:56:Epoch: [2]\tIter: [100]\tTime 0.051 (0.069)\tData 0.000 (0.019)\tLoss 22.597 (22.243)\tPrec@1 28.906 (27.970)\t\n",
      "11-26-20 17:56:Epoch: [2]\tIter: [200]\tTime 0.052 (0.061)\tData 0.000 (0.010)\tLoss 21.397 (22.027)\tPrec@1 32.031 (29.101)\t\n",
      "11-26-20 17:56:Epoch: [2]\tIter: [300]\tTime 0.052 (0.058)\tData 0.000 (0.007)\tLoss 20.885 (21.856)\tPrec@1 32.031 (29.939)\t\n",
      "11-26-20 17:56:Loss 2.710\tPrec@1 31.420\tMiddle1@1 28.380\tMiddle2@1 27.200\tMiddle3@1 29.510\t\n",
      "11-26-20 17:56:Epoch [3] learning rate = 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best:  tensor(31.4200, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11-26-20 17:56:Epoch: [3]\tIter: [0]\tTime 1.887 (1.887)\tData 1.852 (1.852)\tLoss 20.761 (20.761)\tPrec@1 39.062 (39.062)\t\n",
      "11-26-20 17:56:Epoch: [3]\tIter: [100]\tTime 0.051 (0.070)\tData 0.000 (0.019)\tLoss 20.497 (20.536)\tPrec@1 42.188 (37.106)\t\n",
      "11-26-20 17:56:Epoch: [3]\tIter: [200]\tTime 0.052 (0.061)\tData 0.000 (0.010)\tLoss 19.107 (20.392)\tPrec@1 48.438 (37.865)\t\n",
      "11-26-20 17:56:Epoch: [3]\tIter: [300]\tTime 0.052 (0.058)\tData 0.000 (0.006)\tLoss 20.201 (20.240)\tPrec@1 42.969 (38.772)\t\n",
      "11-26-20 17:57:Loss 2.586\tPrec@1 34.970\tMiddle1@1 29.740\tMiddle2@1 29.630\tMiddle3@1 31.380\t\n",
      "11-26-20 17:57:Epoch [4] learning rate = 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best:  tensor(34.9700, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11-26-20 17:57:Epoch: [4]\tIter: [0]\tTime 1.907 (1.907)\tData 1.875 (1.875)\tLoss 19.267 (19.267)\tPrec@1 50.000 (50.000)\t\n",
      "11-26-20 17:57:Epoch: [4]\tIter: [100]\tTime 0.052 (0.070)\tData 0.000 (0.019)\tLoss 20.026 (19.312)\tPrec@1 40.625 (44.787)\t\n",
      "11-26-20 17:57:Epoch: [4]\tIter: [200]\tTime 0.052 (0.061)\tData 0.000 (0.010)\tLoss 18.626 (19.205)\tPrec@1 51.562 (45.099)\t\n",
      "11-26-20 17:57:Epoch: [4]\tIter: [300]\tTime 0.052 (0.058)\tData 0.000 (0.007)\tLoss 18.794 (19.169)\tPrec@1 49.219 (45.473)\t\n",
      "11-26-20 17:57:Loss 1.984\tPrec@1 45.360\tMiddle1@1 33.910\tMiddle2@1 37.720\tMiddle3@1 42.440\t\n",
      "11-26-20 17:57:Epoch [5] learning rate = 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best:  tensor(45.3600, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11-26-20 17:57:Epoch: [5]\tIter: [0]\tTime 1.885 (1.885)\tData 1.849 (1.849)\tLoss 18.660 (18.660)\tPrec@1 48.438 (48.438)\t\n",
      "11-26-20 17:57:Epoch: [5]\tIter: [100]\tTime 0.052 (0.070)\tData 0.000 (0.019)\tLoss 18.527 (18.369)\tPrec@1 47.656 (49.907)\t\n",
      "11-26-20 17:57:Epoch: [5]\tIter: [200]\tTime 0.054 (0.061)\tData 0.000 (0.010)\tLoss 19.358 (18.461)\tPrec@1 50.000 (49.436)\t\n",
      "11-26-20 17:57:Epoch: [5]\tIter: [300]\tTime 0.053 (0.058)\tData 0.000 (0.006)\tLoss 19.363 (18.443)\tPrec@1 49.219 (49.538)\t\n",
      "11-26-20 17:57:Loss 2.070\tPrec@1 44.990\tMiddle1@1 32.640\tMiddle2@1 37.780\tMiddle3@1 40.990\t\n",
      "11-26-20 17:57:Epoch [6] learning rate = 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best:  tensor(45.3600, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11-26-20 17:57:Epoch: [6]\tIter: [0]\tTime 1.885 (1.885)\tData 1.851 (1.851)\tLoss 18.066 (18.066)\tPrec@1 50.781 (50.781)\t\n",
      "11-26-20 17:57:Epoch: [6]\tIter: [100]\tTime 0.054 (0.071)\tData 0.000 (0.019)\tLoss 17.230 (17.933)\tPrec@1 59.375 (53.063)\t\n",
      "11-26-20 17:57:Epoch: [6]\tIter: [200]\tTime 0.053 (0.062)\tData 0.000 (0.010)\tLoss 17.876 (17.961)\tPrec@1 51.562 (52.542)\t\n",
      "11-26-20 17:58:Epoch: [6]\tIter: [300]\tTime 0.054 (0.059)\tData 0.000 (0.006)\tLoss 17.951 (17.924)\tPrec@1 51.562 (52.785)\t\n",
      "11-26-20 17:58:Loss 2.015\tPrec@1 47.540\tMiddle1@1 35.880\tMiddle2@1 39.860\tMiddle3@1 42.780\t\n",
      "11-26-20 17:58:Epoch [7] learning rate = 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best:  tensor(47.5400, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11-26-20 17:58:Epoch: [7]\tIter: [0]\tTime 1.924 (1.924)\tData 1.894 (1.894)\tLoss 17.677 (17.677)\tPrec@1 53.125 (53.125)\t\n",
      "11-26-20 17:58:Epoch: [7]\tIter: [100]\tTime 0.052 (0.071)\tData 0.000 (0.019)\tLoss 16.650 (17.494)\tPrec@1 61.719 (56.165)\t\n",
      "11-26-20 17:58:Epoch: [7]\tIter: [200]\tTime 0.053 (0.062)\tData 0.000 (0.010)\tLoss 17.772 (17.511)\tPrec@1 50.000 (55.826)\t\n",
      "11-26-20 17:58:Epoch: [7]\tIter: [300]\tTime 0.052 (0.059)\tData 0.000 (0.007)\tLoss 17.453 (17.499)\tPrec@1 53.125 (55.666)\t\n",
      "11-26-20 17:58:Loss 1.935\tPrec@1 48.120\tMiddle1@1 38.730\tMiddle2@1 39.660\tMiddle3@1 44.240\t\n",
      "11-26-20 17:58:Epoch [8] learning rate = 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best:  tensor(48.1200, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11-26-20 17:58:Epoch: [8]\tIter: [0]\tTime 1.935 (1.935)\tData 1.899 (1.899)\tLoss 15.987 (15.987)\tPrec@1 65.625 (65.625)\t\n",
      "11-26-20 17:58:Epoch: [8]\tIter: [100]\tTime 0.054 (0.072)\tData 0.000 (0.019)\tLoss 17.288 (17.150)\tPrec@1 59.375 (57.704)\t\n",
      "11-26-20 17:58:Epoch: [8]\tIter: [200]\tTime 0.052 (0.062)\tData 0.000 (0.010)\tLoss 18.339 (17.119)\tPrec@1 47.656 (57.676)\t\n",
      "11-26-20 17:58:Epoch: [8]\tIter: [300]\tTime 0.054 (0.059)\tData 0.000 (0.007)\tLoss 17.297 (17.170)\tPrec@1 58.594 (57.345)\t\n",
      "11-26-20 17:58:Loss 1.891\tPrec@1 48.940\tMiddle1@1 40.010\tMiddle2@1 43.980\tMiddle3@1 45.480\t\n",
      "11-26-20 17:58:Epoch [9] learning rate = 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best:  tensor(48.9400, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11-26-20 17:58:Epoch: [9]\tIter: [0]\tTime 1.915 (1.915)\tData 1.886 (1.886)\tLoss 17.111 (17.111)\tPrec@1 60.938 (60.938)\t\n",
      "11-26-20 17:58:Epoch: [9]\tIter: [100]\tTime 0.054 (0.072)\tData 0.000 (0.019)\tLoss 17.322 (16.775)\tPrec@1 55.469 (59.700)\t\n",
      "11-26-20 17:59:Epoch: [9]\tIter: [200]\tTime 0.054 (0.063)\tData 0.000 (0.010)\tLoss 16.480 (16.818)\tPrec@1 67.188 (59.449)\t\n",
      "11-26-20 17:59:Epoch: [9]\tIter: [300]\tTime 0.052 (0.060)\tData 0.000 (0.007)\tLoss 17.244 (16.850)\tPrec@1 53.906 (59.526)\t\n",
      "11-26-20 17:59:Loss 2.112\tPrec@1 45.820\tMiddle1@1 29.120\tMiddle2@1 34.810\tMiddle3@1 41.540\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best:  tensor(48.9400, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lg_byot",
   "language": "python",
   "name": "lg_byot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
