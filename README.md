# LG_KD
LG paper review QnA session - Knowledge Distillation

### Paper List

1. Distilling the Knowledge in a Neural Network, 15’NIPS (https://arxiv.org/abs/1503.02531)

2. Knowledge Distillation by On-the-Fly Native Ensemble, 18`NIPS, (https://arxiv.org/abs/1806.04606) (ONE/)

3. 


-------
### TAs

- 김성년 ksn4397@kaist.ac.kr
- 최진환 jinhwanchoi@kaist.ac.kr
