{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "\n",
    "import utils\n",
    "import model.net as net\n",
    "import model.resnet as resnet\n",
    "import model.data_loader as data_loader\n",
    "from evaluate import evaluate, evaluate_kd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the datasets...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- done.\n"
     ]
    }
   ],
   "source": [
    "params = utils.Params('./experiments/cnn_distill_subset/params.json')\n",
    "params.cuda = torch.cuda.is_available()\n",
    "params.subset_percent = 0.05\n",
    "random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if params.cuda: \n",
    "    torch.cuda.manual_seed(1)\n",
    "utils.set_logger(os.path.join('./experiments/cnn_distill_subset', 'train.log'))\n",
    "logging.info(\"Loading the datasets...\")\n",
    "train_dl = data_loader.fetch_subset_dataloader('train', params)\n",
    "dev_dl = data_loader.fetch_subset_dataloader('dev', params)\n",
    "logging.info(\"- done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment - model version: cnn_distill\n",
      "Starting training for 100 epoch(s)\n",
      "First, loading the teacher model and computing its outputs...\n"
     ]
    }
   ],
   "source": [
    "# Student model : Conv-5 network\n",
    "model = net.Net(params).cuda() if params.cuda else net.Net(params)\n",
    "optimizer = optim.Adam(model.parameters(), lr=params.learning_rate)\n",
    "metrics = net.metrics\n",
    "\n",
    "# Teacher model : ResNet-18 network (pretrained)\n",
    "teacher_model = resnet.ResNet18()\n",
    "teacher_checkpoint = './experiments/base_resnet18/best.pth.tar'\n",
    "teacher_model = teacher_model.cuda() if params.cuda else teacher_model\n",
    "utils.load_checkpoint(teacher_checkpoint, teacher_model)\n",
    "\n",
    "logging.info(\"Experiment - model version: {}\".format(params.model_version))\n",
    "logging.info(\"Starting training for {} epoch(s)\".format(params.num_epochs))\n",
    "logging.info(\"First, loading the teacher model and computing its outputs...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_kd(outputs, labels, teacher_outputs, params):\n",
    "    \"\"\"\n",
    "    Compute the knowledge-distillation (KD) loss given outputs, labels.\n",
    "    \"Hyperparameters\": temperature and alpha\n",
    "\n",
    "    NOTE: the KL Divergence for PyTorch comparing the softmaxs of teacher\n",
    "    and student expects the input tensor to be log probabilities! See Issue #2\n",
    "    \"\"\"\n",
    "    alpha = params.alpha\n",
    "    T = params.temperature\n",
    "    KD_loss = nn.KLDivLoss()(F.log_softmax(outputs/T, dim=1),\n",
    "                             F.softmax(teacher_outputs/T, dim=1)) * (alpha * T * T) + \\\n",
    "              F.cross_entropy(outputs, labels) * (1. - alpha)\n",
    "\n",
    "    return KD_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "/home/sungnyun/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py:2398: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "- Eval metrics : accuracy: 0.259 ; loss: 0.000\n",
      "Epoch 2/100\n",
      "- Eval metrics : accuracy: 0.363 ; loss: 0.000\n",
      "Epoch 3/100\n",
      "- Eval metrics : accuracy: 0.400 ; loss: 0.000\n",
      "Epoch 4/100\n",
      "- Eval metrics : accuracy: 0.459 ; loss: 0.000\n",
      "Epoch 5/100\n",
      "- Eval metrics : accuracy: 0.516 ; loss: 0.000\n",
      "Epoch 6/100\n",
      "- Eval metrics : accuracy: 0.535 ; loss: 0.000\n",
      "Epoch 7/100\n",
      "- Eval metrics : accuracy: 0.560 ; loss: 0.000\n",
      "Epoch 8/100\n",
      "- Eval metrics : accuracy: 0.591 ; loss: 0.000\n",
      "Epoch 9/100\n",
      "- Eval metrics : accuracy: 0.573 ; loss: 0.000\n",
      "Epoch 10/100\n",
      "- Eval metrics : accuracy: 0.579 ; loss: 0.000\n",
      "Epoch 11/100\n",
      "- Eval metrics : accuracy: 0.610 ; loss: 0.000\n",
      "Epoch 12/100\n",
      "- Eval metrics : accuracy: 0.608 ; loss: 0.000\n",
      "Epoch 13/100\n",
      "- Eval metrics : accuracy: 0.591 ; loss: 0.000\n",
      "Epoch 14/100\n",
      "- Eval metrics : accuracy: 0.596 ; loss: 0.000\n",
      "Epoch 15/100\n",
      "- Eval metrics : accuracy: 0.586 ; loss: 0.000\n",
      "Epoch 16/100\n",
      "- Eval metrics : accuracy: 0.616 ; loss: 0.000\n",
      "Epoch 17/100\n",
      "- Eval metrics : accuracy: 0.608 ; loss: 0.000\n",
      "Epoch 18/100\n",
      "- Eval metrics : accuracy: 0.622 ; loss: 0.000\n",
      "Epoch 19/100\n",
      "- Eval metrics : accuracy: 0.587 ; loss: 0.000\n",
      "Epoch 20/100\n",
      "- Eval metrics : accuracy: 0.648 ; loss: 0.000\n",
      "Epoch 21/100\n",
      "- Eval metrics : accuracy: 0.637 ; loss: 0.000\n",
      "Epoch 22/100\n",
      "- Eval metrics : accuracy: 0.637 ; loss: 0.000\n",
      "Epoch 23/100\n",
      "- Eval metrics : accuracy: 0.628 ; loss: 0.000\n",
      "Epoch 24/100\n",
      "- Eval metrics : accuracy: 0.652 ; loss: 0.000\n",
      "Epoch 25/100\n",
      "- Eval metrics : accuracy: 0.635 ; loss: 0.000\n",
      "Epoch 26/100\n",
      "- Eval metrics : accuracy: 0.615 ; loss: 0.000\n",
      "Epoch 27/100\n",
      "- Eval metrics : accuracy: 0.648 ; loss: 0.000\n",
      "Epoch 28/100\n",
      "- Eval metrics : accuracy: 0.645 ; loss: 0.000\n",
      "Epoch 29/100\n",
      "- Eval metrics : accuracy: 0.644 ; loss: 0.000\n",
      "Epoch 30/100\n",
      "- Eval metrics : accuracy: 0.645 ; loss: 0.000\n",
      "Epoch 31/100\n"
     ]
    }
   ],
   "source": [
    "best_val_acc = 0.0\n",
    "# Teacher model은 학습하는게 아님. -> eval mode로 변경\n",
    "teacher_model.eval()\n",
    "scheduler = StepLR(optimizer, step_size=100, gamma=0.2)\n",
    "val_acc_distill = []\n",
    "\n",
    "for epoch in range(params.num_epochs):\n",
    "    logging.info(\"Epoch {}/{}\".format(epoch + 1, params.num_epochs))\n",
    "    model.train()\n",
    "    summ = []\n",
    "    loss_avg = utils.RunningAverage()\n",
    "    for i, (train_batch, labels_batch) in enumerate(train_dl):\n",
    "        if params.cuda:\n",
    "            train_batch, labels_batch = train_batch.cuda(), labels_batch.cuda()\n",
    "            # teacher model의 output 구하기\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher_model(train_batch)\n",
    "            # student model의 output 구하기\n",
    "            output_batch = model(train_batch)\n",
    "            # KD Loss\n",
    "            loss = loss_fn_kd(output_batch, labels_batch, teacher_outputs, params)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_avg.update(loss.item())\n",
    "    scheduler.step()\n",
    "    val_metrics = evaluate_kd(model, dev_dl, metrics, params)     \n",
    "    val_acc = val_metrics['accuracy']\n",
    "    val_acc_distill.append(val_acc)\n",
    "    is_best = val_acc>=best_val_acc\n",
    "    #print(\"Epoch {}/{} | Val-Acc {:.2f}\".format(epoch+1, params.num_epochs, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = utils.Params('./experiments/base_cnn_subset/params.json')\n",
    "params.cuda = True\n",
    "params.subset_percent = 0.05\n",
    "utils.set_logger(os.path.join('./experiments/base_cnn_subset', 'train.log'))\n",
    "logging.info(\"Loading the datasets...\")\n",
    "train_dl = data_loader.fetch_subset_dataloader('train', params)\n",
    "dev_dl = data_loader.fetch_subset_dataloader('dev', params)\n",
    "logging.info(\"- done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student model : Conv-5 network\n",
    "model = net.Net(params).cuda() if params.cuda else net.Net(params)\n",
    "optimizer = optim.Adam(model.parameters(), lr=params.learning_rate)\n",
    "metrics = net.metrics\n",
    "\n",
    "best_val_acc = 0.0\n",
    "scheduler = StepLR(optimizer, step_size=100, gamma=0.2)\n",
    "val_acc_nodistill = []\n",
    "\n",
    "for epoch in range(params.num_epochs):\n",
    "    logging.info(\"Epoch {}/{}\".format(epoch + 1, params.num_epochs))\n",
    "    model.train()\n",
    "    summ = []\n",
    "    loss_avg = utils.RunningAverage()\n",
    "    for i, (train_batch, labels_batch) in enumerate(train_dl):\n",
    "        if params.cuda:\n",
    "            train_batch, labels_batch = train_batch.cuda(), labels_batch.cuda()\n",
    "            output_batch = model(train_batch)\n",
    "            # CE Loss\n",
    "            loss = nn.CrossEntropyLoss()(output_batch, labels_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_avg.update(loss.item())\n",
    "    scheduler.step()\n",
    "    val_metrics = evaluate_kd(model, dev_dl, metrics, params)     \n",
    "    val_acc = val_metrics['accuracy']\n",
    "    val_acc_nodistill.append(val_acc)\n",
    "    is_best = val_acc>=best_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(1, 51), val_acc_distill)\n",
    "plt.plot(range(1, 51), val_acc_nodistill)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Distill', 'No Distill'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
