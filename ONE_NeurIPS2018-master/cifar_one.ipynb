{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### package import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Training script for CIFAR-10/100\n",
    "Copyright (c) Wei YANG, 2017\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import models.cifar as models\n",
    "import torch.utils.data\n",
    "from loss import KLLoss\n",
    "from torch.utils.data import ConcatDataset\n",
    "from utils import Bar, Logger, AverageMeter, accuracy, mkdir_p, savefig,ramps\n",
    "import pdb\n",
    "import easydict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### arguments settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict({\n",
    "        \"dataset\": 'cifar100',\n",
    "        \"workers\": 4,\n",
    "        \"epochs\": 10,\n",
    "        \"start_epoch\": 0,\n",
    "        \"train_batch\": 128,\n",
    "        \"consistency_rampup\": 80,\n",
    "        \"test_batch\": 100,\n",
    "        \"lr\": 0.1,\n",
    "        \"dropout\": 0,\n",
    "        \"schedule\": [150, 225],\n",
    "        \"gamma\": 0.1,\n",
    "        \"momentum\": 0.9,\n",
    "        \"weight_decay\": 1e-4,\n",
    "        \"checkpoint\": 'checkpoints/cifar100/ONE-32-rampup',\n",
    "        \"resume\": '',\n",
    "        \"arch\": 'one_resnet',\n",
    "        \"momentum\": 0.9,\n",
    "        \"momentum\": 0.9,\n",
    "        'depth': 32,\n",
    "        'cardinality': 8,\n",
    "        'widen_factor': 4,\n",
    "        'growthRate': 12,\n",
    "        'compressionRate': 2,\n",
    "        'layerensemble': False,\n",
    "        'gpu_id': '1',\n",
    "        'manualSeed': None,\n",
    "        'evaluate': None\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seed settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = sorted(name for name in models.__dict__\n",
    "                     if name.islower() and not name.startswith(\"__\")\n",
    "                     and callable(models.__dict__[name]))\n",
    "\n",
    "state = {k: v for k, v in args.items()}\n",
    "\n",
    "# Validate dataset\n",
    "assert args.dataset == 'cifar10' or args.dataset == 'cifar100' or args.dataset == 'svhn', 'Dataset can only be cifar10 or cifar100.'\n",
    "\n",
    "# Use CUDA\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu_id\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Random seed\n",
    "if args.manualSeed is None:\n",
    "    args.manualSeed = random.randint(1, 10000)\n",
    "random.seed(args.manualSeed)\n",
    "torch.manual_seed(args.manualSeed)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed_all(args.manualSeed)\n",
    "\n",
    "best_acc = 0  # best test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global best_acc\n",
    "    start_epoch = args.start_epoch  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "    if not os.path.isdir(args.checkpoint):\n",
    "        mkdir_p(args.checkpoint)\n",
    "\n",
    "    # Data preprocessing\n",
    "    print('==> Preparing dataset %s' % args.dataset)\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    \n",
    "    # Dataset and loader setting\n",
    "    if args.dataset == 'cifar10':\n",
    "        dataloader = datasets.CIFAR10\n",
    "        num_classes = 10\n",
    "    elif args.dataset =='cifar100':\n",
    "        dataloader = datasets.CIFAR100\n",
    "        num_classes = 100\n",
    "    elif args.dataset == 'svhn':\n",
    "        dataloader = datasets.SVHN\n",
    "        num_classes = 10\n",
    "\n",
    "    if args.dataset.startswith('cifar'):\n",
    "\n",
    "        trainset = dataloader(root='./data', train=True, download=True, transform=transform_train)\n",
    "        trainloader = data.DataLoader(trainset, batch_size=args.train_batch, shuffle=True, num_workers=args.workers)\n",
    "        testset = dataloader(root='./data', train=False, download=False, transform=transform_test)\n",
    "        testloader = data.DataLoader(testset, batch_size=args.test_batch, shuffle=False, num_workers=args.workers)\n",
    "    elif args.dataset == 'svhn':\n",
    "        train_dataset = dataloader(root='./data', split='train', download=True, transform=transforms.Compose([\n",
    "                                   # transforms.Scale(opt.imageSize),\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.5, 0.5, 0.5), (0.2, 0.2, 0.2))]))\n",
    "        train_extra_dataset = dataloader(root='./data', split='extra', download=True, transform=transforms.Compose([\n",
    "                                   # transforms.Scale(opt.imageSize),\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.5, 0.5, 0.5), (0.2, 0.2, 0.2))]))\n",
    "        trainset = ConcatDataset([train_dataset, train_extra_dataset])\n",
    "        trainloader = data.DataLoader(trainset, batch_size=args.train_batch, shuffle=True, num_workers=args.workers)\n",
    "        testset = dataloader(root='./data', split='test', download=True, transform=transforms.Compose([\n",
    "                                   # transforms.Scale(opt.imageSize),\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.5, 0.5, 0.5), (0.2, 0.2, 0.2)),\n",
    "                               ]))\n",
    "        testloader = data.DataLoader(testset, batch_size=args.test_batch, shuffle=False, num_workers=args.workers)\n",
    "\n",
    "    else:\n",
    "        print(\"not support dataset\")\n",
    "\n",
    "    # Model Settings\n",
    "    print(\"==> creating model '{}'\".format(args.arch))\n",
    "    if args.arch.endswith('resnext'):\n",
    "        model = models.__dict__[args.arch](\n",
    "            cardinality=args.cardinality,\n",
    "            num_classes=num_classes,\n",
    "            depth=args.depth,\n",
    "            widen_factor=args.widen_factor,\n",
    "            dropRate=args.drop,\n",
    "        )\n",
    "    elif args.arch.endswith('densenet'):\n",
    "\n",
    "        model = models.__dict__[args.arch](\n",
    "            num_classes=num_classes,\n",
    "            depth=args.depth,\n",
    "            growthRate=args.growthRate,\n",
    "            compressionRate=args.compressionRate,\n",
    "            dropRate=args.drop,\n",
    "        )\n",
    "    elif args.arch.startswith('wrn'):\n",
    "        model = models.__dict__[args.arch](\n",
    "            num_classes=num_classes,\n",
    "            depth=args.depth,\n",
    "            widen_factor=args.widen_factor,\n",
    "            dropRate=args.drop,\n",
    "        )\n",
    "    elif args.arch.endswith('resnet'):\n",
    "        model = models.__dict__[args.arch](\n",
    "            num_classes=num_classes,\n",
    "            depth=args.depth,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        model = models.__dict__[args.arch](num_classes=num_classes)\n",
    "\n",
    "    model = torch.nn.DataParallel(model).cuda()\n",
    "    cudnn.benchmark = True\n",
    "    print('    Total params: %.2fM' % (sum(p.numel() for p in model.parameters()) / 1000000.0))\n",
    "    \n",
    "    # Use two kinds of loss functions: CE and KL\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    criterion_kl = KLLoss().cuda()\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "\n",
    "    # Resume\n",
    "    title = 'cifar' + args.arch\n",
    "\n",
    "    if args.resume or args.evaluate:\n",
    "        # Load checkpoint.\n",
    "\n",
    "        print('==> Resuming from checkpoint..')\n",
    "        assert os.path.isfile(args.resume), 'Error: no checkpoint directory found!'\n",
    "        args.checkpoint = os.path.dirname(args.resume)\n",
    "        checkpoint = torch.load(args.resume)\n",
    "        best_acc = checkpoint['best_acc']\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        logger = Logger(os.path.join(args.checkpoint, 'log.txt'), title=title, resume=True)\n",
    "    else:\n",
    "        logger = Logger(os.path.join(args.checkpoint, 'log.txt'), title=title)\n",
    "        logger.set_names(['TAcc_1', 'VAcc_1','TAcc_2','VAcc_2','TAcc_3','VAcc_3','TAcc_e','VAcc_e'])\n",
    "\n",
    "    if args.evaluate:\n",
    "        print('\\nEvaluation only')\n",
    "        test_loss, test_acc = test(testloader, model, use_cuda)\n",
    "        print(' Test Loss:  %.8f, Test Acc:  %.2f' % (test_loss, test_acc))\n",
    "        return\n",
    "\n",
    "    # Train and val\n",
    "    for epoch in range(start_epoch, args.epochs):\n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "        print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, args.epochs, state['lr']))\n",
    "\n",
    "        train_loss, train_acc_1,train_acc_2,train_acc_3,train_acc_en = train(trainloader, model, criterion, criterion_kl, optimizer, epoch,\n",
    "                                      use_cuda)\n",
    "        test_loss, test_acc_1,test_acc_2,test_acc_3,test_acc_en = test(testloader, model, use_cuda)\n",
    "\n",
    "        # append logger file\n",
    "        logger.append([train_acc_1, test_acc_1,train_acc_2,test_acc_2,train_acc_3,test_acc_3,train_acc_en,test_acc_en])\n",
    "\n",
    "        # save model\n",
    "        is_best = test_acc_1 > best_acc\n",
    "        best_acc = max(test_acc_1, best_acc)\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'acc': test_acc_1,\n",
    "            'best_acc': best_acc,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, is_best, checkpoint=args.checkpoint)\n",
    "\n",
    "    logger.close()\n",
    "    logger.plot()\n",
    "    savefig(os.path.join(args.checkpoint, 'log.eps'))\n",
    "\n",
    "    print('Best acc:')\n",
    "    print(best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainloader, model, criterion, criterion_kl, optimizer, epoch, use_cuda):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    # Use three branches c1~c3, AverageMeter - save current and average values\n",
    "    losses = AverageMeter()\n",
    "    losses_kl = AverageMeter()\n",
    "    top1_c1 = AverageMeter()\n",
    "    top5_c1 = AverageMeter()\n",
    "    top1_c2 = AverageMeter()\n",
    "    top5_c2 = AverageMeter()\n",
    "    top1_c3 = AverageMeter()\n",
    "    top5_c3 = AverageMeter()\n",
    "    top1_t = AverageMeter()\n",
    "    top5_t = AverageMeter()\n",
    "\n",
    "\n",
    "    bar = Bar('Processing', max=len(trainloader))\n",
    "    consistency_weight = get_current_consistency_weight(epoch)\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda(non_blocking=True)\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "        \n",
    "        # output1~3 : branch output, output4 : teacher output (ensembled)\n",
    "        outputs1, outputs2, outputs3, outputs4 = model(inputs)\n",
    "        \n",
    "        # CE loss on each branch and teacher\n",
    "        loss_cross = criterion(outputs1, targets) + criterion(outputs2, targets) + criterion(outputs3,targets) + criterion(\n",
    "                 outputs4, targets)\n",
    "        \n",
    "        # KL loss between output4(teacher) and output1~3(branch)\n",
    "        loss_kl = consistency_weight*(criterion_kl(outputs1, outputs4) +criterion_kl(outputs2,outputs4)+criterion_kl(outputs3,outputs4))\n",
    "        \n",
    "        # training process update\n",
    "        prec1_t, prec5_t = accuracy(outputs4.data, targets.data, topk=(1, 5))\n",
    "        prec1_c1, prec5_c1 = accuracy(outputs1.data, targets.data, topk=(1, 5))\n",
    "        prec1_c2, prec5_c2 = accuracy(outputs2.data, targets.data, topk=(1, 5))\n",
    "        prec1_c3, prec5_c3 = accuracy(outputs3.data, targets.data, topk=(1, 5))\n",
    "        top1_c1.update(prec1_c1.item(), inputs.size(0))\n",
    "        top5_c1.update(prec5_c1.item(), inputs.size(0))\n",
    "        loss = loss_cross+loss_kl\n",
    "        losses_kl.update(loss_kl.data, inputs.size(0))\n",
    "        losses.update(loss.data, inputs.size(0))\n",
    "        top1_c2.update(prec1_c2.item(), inputs.size(0))\n",
    "        top5_c2.update(prec5_c2.item(), inputs.size(0))\n",
    "        top1_c3.update(prec1_c3.item(), inputs.size(0))\n",
    "        top5_c3.update(prec5_c3.item(), inputs.size(0))\n",
    "        top1_t.update(prec1_t.item(), inputs.size(0))\n",
    "        top5_t.update(prec5_t.item(), inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 50 == 0:\n",
    "            print('({batch}/{size})   || Loss: {loss:.4f} |LossKL: {losses_kl:.4f} | top1_C1: {top1_C1: .4f} | top1_C2: {top1_C2: .4f}|top1_C3: {top1_C3: .4f}| top1_t: {top1_t: .4f} '.format(\n",
    "            batch=batch_idx + 1,\n",
    "            size=len(trainloader),\n",
    "            loss=losses.avg,\n",
    "            losses_kl=losses_kl.avg,\n",
    "            top1_C1=top1_c1.avg,\n",
    "            top1_C2=top1_c2.avg,\n",
    "            top1_C3=top1_c3.avg,\n",
    "            top1_t=top1_t.avg,\n",
    "        ))\n",
    "\n",
    "#         bar.suffix = '({batch}/{size})   || Loss: {loss:.4f} |LossKL: {losses_kl:.4f} | top1_C1: {top1_C1: .4f} | top1_C2: {top1_C2: .4f}|top1_C3: {top1_C3: .4f}| top1_t: {top1_t: .4f} '.format(\n",
    "#             batch=batch_idx + 1,\n",
    "#             size=len(trainloader),\n",
    "#             loss=losses.avg,\n",
    "#             losses_kl=losses_kl.avg,\n",
    "#             top1_C1=top1_c1.avg,\n",
    "#             top1_C2=top1_c2.avg,\n",
    "#             top1_C3=top1_c3.avg,\n",
    "#             top1_t=top1_t.avg,\n",
    "#         )\n",
    "#         bar.next()\n",
    "#     bar.finish()\n",
    "    return (losses.avg,top1_c1.avg,top1_c2.avg,top1_c3.avg,top1_t.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def test(testloader, model, use_cuda):\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1_c1 = AverageMeter()\n",
    "    top1_c2 = AverageMeter()\n",
    "    top1_c3 = AverageMeter()\n",
    "    top1_avg= AverageMeter()\n",
    "    top1_t = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    bar = Bar('Processing', max=len(testloader))\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n",
    "\n",
    "        # Branch (1~3) and teacher (4)\n",
    "        outputs1, outputs2, outputs3, outputs4 = model(inputs)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1_c1, _ = accuracy(outputs1.data, targets.data, topk=(1, 5))\n",
    "        prec1_c2, _ = accuracy(outputs2.data, targets.data, topk=(1, 5))\n",
    "        prec1_c3, _ = accuracy(outputs3.data, targets.data, topk=(1, 5))\n",
    "        prec1_en, _ = accuracy(outputs4.data, targets.data, topk=(1, 5))\n",
    "        top1_c1.update(prec1_c1.item(), inputs.size(0))\n",
    "        top1_c2.update(prec1_c2.item(), inputs.size(0))\n",
    "        top1_c3.update(prec1_c3.item(), inputs.size(0))\n",
    "        top1_avg.update((prec1_c1.item()+prec1_c2.item()+prec1_c3.item())/3, inputs.size(0))\n",
    "        top1_t.update(prec1_en.item(), inputs.size(0))\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx % 50 == 0 or batch_idx == len(testloader):\n",
    "            print( '({batch}/{size})| Loss: {loss: .4f} | top1_C1: {top1_C1: .4f} |top1_C2: {top1_C2: .4f}|top1_C3: {top1_C3:.4f} |top1_t: {top1_t: .4f}'.format(\n",
    "                batch=batch_idx + 1,\n",
    "                size=len(testloader),\n",
    "                loss=losses.avg,\n",
    "                top1_C1=top1_c1.avg,\n",
    "                top1_C2=top1_c2.avg,\n",
    "                top1_C3=top1_c3.avg,\n",
    "                top1_t=top1_t.avg,\n",
    "            ))\n",
    "#         bar.suffix = '({batch}/{size})| Loss: {loss: .4f} | top1_C1: {top1_C1: .4f} |top1_C2: {top1_C2: .4f}|top1_C3: {top1_C3:.4f} |top1_t: {top1_t: .4f}'.format(\n",
    "#             batch=batch_idx + 1,\n",
    "#             size=len(testloader),\n",
    "#             loss=losses.avg,\n",
    "#             top1_C1=top1_c1.avg,\n",
    "#             top1_C2=top1_c2.avg,\n",
    "#             top1_C3=top1_c3.avg,\n",
    "#             top1_t=top1_t.avg,\n",
    "#         )\n",
    "#         bar.next()\n",
    "\n",
    "#     bar.finish()\n",
    "    return (losses.avg, top1_c1.avg,top1_c2.avg,top1_c3.avg,top1_t.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def get_current_consistency_weight(epoch):\n",
    "    # Consistency ramp-up from https://arxiv.org/abs/1610.02242\n",
    "    return ramps.sigmoid_rampup(epoch, args.consistency_rampup)\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    global state\n",
    "    if epoch in args.schedule:\n",
    "        state['lr'] *= args.gamma\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing dataset cifar100\n",
      "Files already downloaded and verified\n",
      "==> creating model 'one_resnet'\n",
      "    Total params: 1.19M\n",
      "\n",
      "Epoch: [1 | 10] LR: 0.100000\n",
      "(1/391)   || Loss: 19.5092 |LossKL: 0.0040 | top1_C1:  0.0000 | top1_C2:  1.5625|top1_C3:  1.5625| top1_t:  0.7812 \n",
      "(51/391)   || Loss: 18.3836 |LossKL: 0.0009 | top1_C1:  2.3131 | top1_C2:  2.2672|top1_C3:  1.7463| top1_t:  2.4510 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-ecad36a91a63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-df5c55ed1933>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         train_loss, train_acc_1,train_acc_2,train_acc_3,train_acc_en = train(trainloader, model, criterion, criterion_kl, optimizer, epoch,\n\u001b[0;32m--> 135\u001b[0;31m                                       use_cuda)\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_acc_2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_acc_3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_acc_en\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-ecad36a91a63>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(trainloader, model, criterion, criterion_kl, optimizer, epoch, use_cuda)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0moutputs1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         loss_cross = criterion(outputs1, targets) + criterion(outputs2, targets) + criterion(outputs3,targets) + criterion(\n",
      "\u001b[0;32m~/anaconda3/envs/lg_one/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lg_one/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lg_one/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/LG_KD/ONE_NeurIPS2018-master/models/cifar/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0mx_3_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 8x8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mx_3_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mx_3_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lg_one/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lg_one/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lg_one/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/LG_KD/ONE_NeurIPS2018-master/models/cifar/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lg_one/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lg_one/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lg_one/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1668\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   1669\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1670\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1671\u001b[0m     )\n\u001b[1;32m   1672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lg_one",
   "language": "python",
   "name": "lg_one"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
